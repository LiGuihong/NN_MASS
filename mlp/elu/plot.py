import numpy as np
from matplotlib import pyplot as plt


relu_acc=np.loadtxt('relu_v1_acc.csv',delimiter=',')
relu_loss=np.loadtxt('relu_v1_loss.csv',delimiter=',')
semi_acc=np.loadtxt('semi_linear_v1_acc.csv',delimiter=',')
semi_loss=np.loadtxt('semi_linear_v1_loss.csv',delimiter=',')
tanh_acc=np.loadtxt('tanh_v1_acc.csv',delimiter=',')
tanh_loss=np.loadtxt('tanh_v1_loss.csv',delimiter=',')
elu_acc=np.loadtxt('elu_v1_acc.csv',delimiter=',')
elu_loss=np.loadtxt('elu_v1_loss.csv',delimiter=',')
leaky_acc=np.loadtxt('leaky_relu_v1_acc.csv',delimiter=',')
leaky_loss=np.loadtxt('leaky_relu_v1_loss.csv',delimiter=',')

net_width=(np.arange(16,dtype=int)*2+10)
net_depth=(np.arange(6,dtype=int)*2+3)


plt.figure(1)
plt.plot(net_width,relu_acc[0])
plt.plot(net_width,semi_acc[0])
plt.plot(net_width,tanh_acc[0])
plt.plot(net_width,elu_acc[0])
plt.plot(net_width,leaky_acc[0])
plt.title('Test accuracy with different activation functions (depth=3)')
plt.xlabel('Width of networks')
plt.ylabel('Test accuracy')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_acc_3.png',dpi=600,format='png')
plt.show()
plt.figure(2)
plt.plot(net_width,relu_loss[0])
plt.plot(net_width,semi_loss[0])
plt.plot(net_width,tanh_loss[0])
plt.plot(net_width,elu_loss[0])
plt.plot(net_width,leaky_loss[0])
plt.title('Test loss with different activation functions (depth=3)')
plt.xlabel('Width of networks')
plt.ylabel('Test loss')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_loss_3.png',dpi=600,format='png')
plt.show()


plt.figure(3)
plt.plot(net_width,relu_acc[1])
plt.plot(net_width,semi_acc[1])
plt.plot(net_width,tanh_acc[0])
plt.plot(net_width,elu_acc[1])
plt.plot(net_width,leaky_acc[1])
plt.title('Test accuracy with different activation functions (depth=5)')
plt.xlabel('Width of networks')
plt.ylabel('Test accuracy')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_acc_5.png',dpi=600,format='png')
plt.figure(4)
plt.plot(net_width,relu_loss[1])
plt.plot(net_width,semi_loss[1])
plt.plot(net_width,tanh_loss[1])
plt.plot(net_width,elu_loss[1])
plt.plot(net_width,leaky_loss[1])
plt.title('Test loss with different activation functions (depth=5)')
plt.xlabel('Width of networks')
plt.ylabel('Test loss')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_loss_5.png',dpi=600,format='png')
plt.show()


plt.figure(5)
plt.plot(net_width,relu_acc[2])
plt.plot(net_width,semi_acc[2])
plt.plot(net_width,tanh_acc[2])
plt.plot(net_width,elu_acc[2])
plt.plot(net_width,leaky_acc[2])
plt.title('Test accuracy with different activation functions (depth=7)')
plt.xlabel('Width of networks')
plt.ylabel('Test accuracy')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_acc_7.png',dpi=600,format='png')
plt.show()
plt.figure(6)
plt.plot(net_width,relu_loss[2])
plt.plot(net_width,semi_loss[2])
plt.plot(net_width,tanh_loss[2])
plt.plot(net_width,elu_loss[2])
plt.plot(net_width,leaky_loss[2])
plt.title('Test loss with different activation functions (depth=7)')
plt.xlabel('Width of networks')
plt.ylabel('Test loss')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_loss_7.png',dpi=600,format='png')
plt.show()


plt.figure(7)
plt.plot(net_width,relu_acc[3])
plt.plot(net_width,semi_acc[3])
plt.plot(net_width,tanh_acc[3])
plt.plot(net_width,elu_acc[3])
plt.plot(net_width,leaky_acc[3])
plt.title('Test accuracy with different activation functions (depth=9)')
plt.xlabel('Width of networks')
plt.ylabel('Test accuracy')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_acc_9.png',dpi=600,format='png')
plt.show()
plt.figure(8)
plt.plot(net_width,relu_loss[3])
plt.plot(net_width,semi_loss[3])
plt.plot(net_width,tanh_loss[3])
plt.plot(net_width,elu_loss[3])
plt.plot(net_width,leaky_loss[3])
plt.title('Test loss with different activation functions (depth=9)')
plt.xlabel('Width of networks')
plt.ylabel('Test loss')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_loss_9.png',dpi=600,format='png')
plt.show()


plt.figure(9)
plt.plot(net_width,relu_acc[4])
plt.plot(net_width,semi_acc[4])
plt.plot(net_width,tanh_acc[4])
plt.plot(net_width,elu_acc[4])
plt.plot(net_width,leaky_acc[4])
plt.title('Test accuracy with different activation functions (depth=11)')
plt.xlabel('Width of networks')
plt.ylabel('Test accuracy')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_acc_11.png',dpi=600,format='png')
plt.show()
plt.figure(10)
plt.plot(net_width,relu_loss[4])
plt.plot(net_width,semi_loss[4])
plt.plot(net_width,tanh_loss[4])
plt.plot(net_width,elu_loss[4])
plt.plot(net_width,leaky_loss[4])
plt.title('Test loss with different activation functions (depth=11)')
plt.xlabel('Width of networks')
plt.ylabel('Test loss')
plt.legend(['relu','semi_linear','tanh','elu','leaky'])
plt.savefig('fig/act_loss_11.png',dpi=600,format='png')
plt.show()
